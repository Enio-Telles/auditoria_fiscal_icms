#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Setup do Sistema de IA Local com Ollama
======================================

Script para configurar e instalar Ollama com modelos LLM locais
"""

import subprocess
import requests
import time
import platform
from pathlib import Path


def check_ollama_installed():
    """Verifica se Ollama est√° instalado"""
    try:
        result = subprocess.run(
            ["ollama", "--version"], capture_output=True, text=True, timeout=5
        )
        return result.returncode == 0
    except Exception:
        return False


def check_ollama_running():
    """Verifica se Ollama est√° rodando"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        return response.status_code == 200
    except Exception:
        return False


def start_ollama():
    """Inicia o servi√ßo Ollama"""
    try:
        if platform.system() == "Windows":
            # No Windows, Ollama roda como servi√ßo
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
        else:
            # Linux/Mac
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )

        # Aguardar inicializa√ß√£o
        for i in range(30):  # 30 segundos m√°ximo
            if check_ollama_running():
                return True
            time.sleep(1)

        return False
    except Exception as e:
        print(f"‚ùå Erro ao iniciar Ollama: {e}")
        return False


def install_model(model_name: str, timeout: int = 600):
    """Instala um modelo espec√≠fico"""
    print(f"üì¶ Baixando modelo {model_name}...")
    print("   (Isso pode demorar alguns minutos na primeira vez)")

    try:
        # Usar ollama pull diretamente
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Aguardar conclus√£o com timeout
        try:
            stdout, stderr = process.communicate(timeout=timeout)
            if process.returncode == 0:
                print(f"‚úÖ Modelo {model_name} instalado com sucesso!")
                return True
            else:
                print(f"‚ùå Erro ao instalar {model_name}: {stderr}")
                return False
        except subprocess.TimeoutExpired:
            process.kill()
            print(f"‚è∞ Timeout ao instalar {model_name}")
            return False

    except Exception as e:
        print(f"‚ùå Erro ao instalar modelo {model_name}: {e}")
        return False


def setup_ollama_system():
    """Configura sistema completo do Ollama"""
    print("ü§ñ SETUP DO SISTEMA DE IA LOCAL COM OLLAMA")
    print("=" * 60)

    # 1. Verificar se Ollama est√° instalado
    print("\n1Ô∏è‚É£ Verificando instala√ß√£o do Ollama...")
    if not check_ollama_installed():
        print("‚ùå Ollama n√£o encontrado!")
        print("\nüì• INSTRU√á√ïES DE INSTALA√á√ÉO:")
        print("=" * 40)

        if platform.system() == "Windows":
            print("Windows:")
            print("1. Baixe o instalador: https://ollama.ai/download/windows")
            print("2. Execute o instalador e siga as instru√ß√µes")
            print("3. Reinicie o terminal e execute este script novamente")
        else:
            print("Linux/Mac:")
            print("curl -fsSL https://ollama.ai/install.sh | sh")

        return False

    print("‚úÖ Ollama instalado!")

    # 2. Verificar se est√° rodando
    print("\n2Ô∏è‚É£ Verificando se Ollama est√° rodando...")
    if not check_ollama_running():
        print("‚ö†Ô∏è Ollama n√£o est√° rodando, tentando iniciar...")
        if start_ollama():
            print("‚úÖ Ollama iniciado com sucesso!")
        else:
            print("‚ùå Falha ao iniciar Ollama")
            print("üí° Tente executar manualmente: ollama serve")
            return False
    else:
        print("‚úÖ Ollama j√° est√° rodando!")

    # 3. Verificar modelos dispon√≠veis
    print("\n3Ô∏è‚É£ Verificando modelos dispon√≠veis...")
    try:
        response = requests.get("http://localhost:11434/api/tags")
        models = response.json().get("models", [])
        model_names = [model["name"] for model in models]

        if model_names:
            print(f"üìã Modelos j√° instalados: {', '.join(model_names)}")
        else:
            print("üìã Nenhum modelo instalado ainda")

    except Exception as e:
        print(f"‚ùå Erro ao verificar modelos: {e}")
        return False

    # 4. Instalar modelos recomendados
    print("\n4Ô∏è‚É£ Instalando modelos recomendados...")

    modelos_recomendados = [
        ("llama3.1:8b", "Modelo principal - mais preciso"),
        ("llama2:7b", "Modelo backup - mais r√°pido"),
    ]

    modelos_instalados = []

    for modelo, descricao in modelos_recomendados:
        # Verificar se j√° est√° instalado
        if any(modelo in name for name in model_names):
            print(f"‚úÖ {modelo} j√° instalado - {descricao}")
            modelos_instalados.append(modelo)
            continue

        print(f"\nüì¶ Instalando {modelo} - {descricao}")
        if install_model(modelo):
            modelos_instalados.append(modelo)
        else:
            print(f"‚ö†Ô∏è Falha ao instalar {modelo}, continuando...")

    # 5. Teste b√°sico
    print("\n5Ô∏è‚É£ Testando sistema...")
    if modelos_instalados:
        modelo_teste = modelos_instalados[0]
        print(f"üß™ Testando modelo {modelo_teste}...")

        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": modelo_teste,
                    "prompt": "Responda apenas: OK",
                    "stream": False,
                    "options": {"num_predict": 10},
                },
                timeout=30,
            )

            if response.status_code == 200:
                result = response.json()
                resposta = result.get("response", "").strip()
                print(f"‚úÖ Teste bem-sucedido! Resposta: '{resposta}'")

                # Salvar configura√ß√£o
                config_data = {
                    "ollama_status": "ready",
                    "modelos_instalados": modelos_instalados,
                    "modelo_principal": modelos_instalados[0],
                    "data_setup": time.strftime("%Y-%m-%d %H:%M:%S"),
                }

                config_file = Path("config/ollama_config.json")
                config_file.parent.mkdir(parents=True, exist_ok=True)

                import json

                with open(config_file, "w") as f:
                    json.dump(config_data, f, indent=2)

                print(f"üìÑ Configura√ß√£o salva em: {config_file}")

            else:
                print(f"‚ùå Teste falhou: HTTP {response.status_code}")
                return False

        except Exception as e:
            print(f"‚ùå Erro no teste: {e}")
            return False

    else:
        print("‚ùå Nenhum modelo foi instalado com sucesso")
        return False

    # 6. Resultado final
    print("\nüéâ SETUP CONCLU√çDO COM SUCESSO!")
    print("=" * 60)
    print("‚úÖ Ollama rodando em: http://localhost:11434")
    print(f"‚úÖ Modelos dispon√≠veis: {', '.join(modelos_instalados)}")
    print("‚úÖ Sistema pronto para classifica√ß√£o de produtos!")
    print("\nüí° Para testar:")
    print("   python src/auditoria_icms/ai/local_llm_classifier.py --test")

    return True


def check_system_status():
    """Verifica status atual do sistema"""
    print("üìä STATUS DO SISTEMA DE IA LOCAL")
    print("=" * 40)

    # Ollama instalado?
    ollama_installed = check_ollama_installed()
    print(f"Ollama instalado: {'‚úÖ SIM' if ollama_installed else '‚ùå N√ÉO'}")

    if not ollama_installed:
        print("‚ùå Execute o setup primeiro!")
        return

    # Ollama rodando?
    ollama_running = check_ollama_running()
    print(f"Ollama rodando: {'‚úÖ SIM' if ollama_running else '‚ùå N√ÉO'}")

    if not ollama_running:
        print("üí° Execute: ollama serve")
        return

    # Modelos dispon√≠veis
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            if models:
                print(f"Modelos instalados: ‚úÖ {len(models)}")
                for model in models:
                    size_gb = model.get("size", 0) / (1024**3)
                    print(f"  ‚Ä¢ {model['name']} ({size_gb:.1f}GB)")
            else:
                print("Modelos instalados: ‚ùå NENHUM")
        else:
            print("‚ùå Erro ao conectar com Ollama")
    except Exception as e:
        print(f"‚ùå Erro: {e}")

    # Teste r√°pido
    print("\nüß™ Teste r√°pido...")
    try:
        from src.auditoria_icms.ai.local_llm_classifier import LocalLLMClassifier

        classifier = LocalLLMClassifier()

        if classifier.check_ollama_connection():
            print("‚úÖ Conex√£o OK")

            # Teste de classifica√ß√£o
            result = classifier.classify_product("teste", "Notebook Dell")
            if result.ncm_sugerido != "0000.00.00":
                print(f"‚úÖ Classifica√ß√£o OK: {result.ncm_sugerido}")
            else:
                print("‚ö†Ô∏è Classifica√ß√£o com problemas")
        else:
            print("‚ùå Falha na conex√£o")

    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Setup do Sistema de IA Local")
    parser.add_argument("--setup", action="store_true", help="Executar setup completo")
    parser.add_argument("--status", action="store_true", help="Verificar status")
    parser.add_argument("--start", action="store_true", help="Iniciar Ollama")

    args = parser.parse_args()

    if args.setup:
        setup_ollama_system()
    elif args.status:
        check_system_status()
    elif args.start:
        if start_ollama():
            print("‚úÖ Ollama iniciado!")
        else:
            print("‚ùå Falha ao iniciar Ollama")
    else:
        print("ü§ñ Sistema de IA Local - Ollama")
        print("=" * 40)
        print("Op√ß√µes:")
        print("  --setup   Configurar sistema completo")
        print("  --status  Verificar status atual")
        print("  --start   Iniciar servi√ßo Ollama")
        print("\nExemplo: python setup_ollama.py --setup")
